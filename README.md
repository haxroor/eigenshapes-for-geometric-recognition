# Geometric Shape Recognition using PCA ("Eigen-Shapes")

This small project, developed for a university Data Mining course, implements a pipeline for classifying 2D geometric shapes (ellipses, rectangles, segments, and crosses) from noisy point clouds.

The core idea is to transform a classic geometric problem into an image classification task by employing a feature engineering approach based on Principal Component Analysis (PCA), inspired by the "Eigenfaces" concept.

## Methodology: The "Eigen-Shapes" Pipeline

Instead of relying on traditional geometric features (like area, perimeter, or angles), which are highly sensitive to noise, this project uses a preprocessing pipeline to "normalize" each shape and make it robust to translation, rotation, scale, and noise.

The process, implemented in the `main.ipynb` notebook, follows these steps:

1.  **1D Point Smoothing (Savitzky-Golay)**: A Savitzky-Golay filter is applied directly to the (x, y) coordinates of the points. This reduces high-frequency noise by smoothing the shape's contour while preserving its main characteristics.

2.  **PCA-based Alignment (Rotation Invariance)**: A preliminary PCA is performed on the cleaned points. Its purpose is not to reduce dimensionality but to find the shape's principal axes of inertia. The points are then projected onto this new coordinate system, effectively aligning every shape into a "canonical" pose (e.g., with its longest side oriented horizontally). This step is crucial for achieving rotation invariance.

3.  **Rasterization**: The aligned and smoothed points are converted into a fixed-size binary image (e.g., 32x32 pixels). This function centers the shape, scales it to fit within the image, and draws its contour. This step transforms the data from a vector domain (points) to an image domain (pixels).

4.  **2D Gaussian Blur**: A light Gaussian filter is applied to the rasterized image to smooth out aliasing artifacts and create a "softer" representation.

5.  **Final Feature Extraction (PCA on Images)**: A second, definitive PCA is applied to the flattened pixel vectors of the images. This PCA reduces dimensionality (e.g., from 1024 to 25 components) by extracting the "eigen-shapes"—the principal components that capture the most variance in the image space. The resulting compact and information-dense feature vector is then used as input for the classifiers.

### Why PCA on Images and Not Directly on Points?

One might wonder why the final PCA is applied to rasterized images instead of directly to the aligned point coordinates. The rasterization step is a key part of the feature engineering process for several reasons:

1.  **Implicit Feature Extraction**: Converting points to a fixed-size grid (an image) implicitly extracts richer spatial information. The presence or absence of a point becomes a feature, but so do the relationships between pixels. The "empty" space (black pixels) around the shape also becomes part of the pattern that the PCA can learn. Even the background becomes a feature. This creates a much higher-dimensional and more descriptive representation.

2.  **Handles Varying Point Counts**: The number of points describing each shape can vary. Rasterization naturally solves this problem by mapping any number of points onto a standardized, fixed-size feature space (the `32x32` pixel grid).

3.  **Noise Resilience**: By rasterizing and then blurring, the impact of small amounts of noise on individual point coordinates is significantly reduced. The overall "shape" captured by the pixel grid is more stable than the exact location of each point. A slightly displaced point will likely still activate the same or an adjacent pixel, resulting in a very similar image.

In short, transforming the point cloud into an image enriches the dataset, making it more robust and suitable for a powerful pattern-extraction technique like PCA.

## Project Structure

```
geometric-shape-recognition-pca/
├── data/
│   └── dataset.json         # Dataset generated by the script
├── generate_dataset.py      # Script to create the synthetic dataset
├── main.ipynb               # Jupyter notebook with analysis and models
├── README.md                # This file
└── requirements.txt         # Project's Python dependencies
```

## How to Run the Project

To replicate the analysis, follow these steps:

**1. Prerequisites:**
Ensure you have Python 3.x and `pip` installed.

**2. Clone the Repository:**
```bash
git clone https://github.com/your-username/geometric-shape-recognition-pca.git
cd geometric-shape-recognition-pca
```

**3. (Recommended) Create a Virtual Environment:**
```bash
python -m venv venv
# On Windows:
# venv\Scripts\activate
# On macOS/Linux:
# source venv/bin/activate
```

**4. Install Dependencies:**
```bash
pip install -r requirements.txt
```

**5. Generate the Dataset:**
The `generate_dataset.py` script creates a synthetic dataset with predefined parameters and saves it to `data/dataset.json`. Run this before launching the notebook.
```bash
python generate_dataset.py
```

**6. Launch Jupyter Notebook:**
```bash
jupyter notebook main.ipynb
```
You can now run all the cells in the notebook to see the analysis, model training, and results.

## Models and Evaluation

Two classifiers were trained on the features extracted by the PCA pipeline:
*   **Random Forest Classifier**
*   **Support Vector Classifier (SVC)**

The system's robustness was evaluated by testing a single model (trained on high-noise data) against variations in:
-   **Noise Level** in the test dataset.
-   **Number of Shapes** per class.
-   **Number of Points** describing each shape.

## Key Results

The pipeline proved to be extremely robust, leading to very high classification performance.

-   **Noise Robustness**: The accuracy of both models remains **above 99%** even with significant noise levels, demonstrating the exceptional effectiveness of the pipeline in cleaning and normalizing the data.
-   **Data Size Robustness**: Performance is excellent even with a limited number of training shapes and reaches a plateau very quickly.
-   **Point Density Robustness**: The system is resilient to the number of points describing each shape.

The near-perfect performance does not indicate an error or *data leakage* but is a direct consequence of a feature engineering pipeline that effectively solves invariance and noise issues upfront, making the final classification task much simpler for the standard models.